{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d05338",
   "metadata": {},
   "source": [
    "# 1. Scrape the details of most viewed videos on YouTube from Wikipedia. Url \n",
    "= https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos You need to find following details: A)\n",
    "Rank \n",
    "B) Name \n",
    "C) Artist \n",
    "D) Upload date \n",
    "E) Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b43fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "url='https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "table=soup.find('table',class_='wikitable sortable')\n",
    "print(table)\n",
    "rows=table.find_all('tr')\n",
    "for row in table.find_all('tr')[1:]:\n",
    "     cells=row.find_all('td')\n",
    "     rank=cells[0].text.strip()\n",
    "     name=cells[1].text.strip()\n",
    "     artist=cells[2].text.strip()\n",
    "     upload_date=cells[4].text.strip()\n",
    "     views=cells[3].text.strip()\n",
    "     print(f'Rank:{rank}')\n",
    "     print(f'Name:{name}')\n",
    "     print(f'Artist:{artist}')\n",
    "     print(f'Upload Date:{upload_date}')\n",
    "     print(f'Views:{views}')\n",
    "     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeeeffb",
   "metadata": {},
   "source": [
    "# 2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv. \n",
    "Url = https://www.bcci.tv/. \n",
    "You need to find following details: \n",
    "A) Series \n",
    "B) Place \n",
    "C) Date \n",
    "D) Time \n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767cccf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://www.bcci.tv/'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "fixture_elements=soup.find_all('div',class_='fixtures')\n",
    "fixtures=[]\n",
    "for element in fixture_elements:\n",
    "    series=element.find('span',class_='fixture_format-strip--suffix').text.strip()\n",
    "    place=element.find('p',class_='fixture__additional-info').text.strip()\n",
    "    date=element.find('span',class_='fixture__datetime').text.strip()\n",
    "    time=element.find('span',class_='fixture__time').text.strip()\n",
    "    fixture={'Series':series,'Place':place,'Date':date,'Time':time}\n",
    "    fixtures.append(fixture)\n",
    "    print(fixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa30786",
   "metadata": {},
   "source": [
    "# 3. Scrape the details of State-wise GDP of India from statisticstime.com. \n",
    "Url = http://statisticstimes.com/ \n",
    "You have to find following details: A) Rank \n",
    "B) State \n",
    "C) GSDP(18-19)- at current prices \n",
    "D) GSDP(19-20)- at current prices \n",
    "E) Share(18-19) \n",
    "F) GDP($ billion) \n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf881aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url='http://statisticstimes.com/'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "economy_link=soup.find('a',text='Economy')\n",
    "economy_url=economy_link.get('href')\n",
    "economy_response=requests.get(economy_url)\n",
    "economy_soup=BeautifulSoup(economy_response.content,'html.parser')\n",
    "Gdp_table=economy_soup.find('table',class_='display')\n",
    "data=[]\n",
    "rows=Gdp_table.find_all('tr')\n",
    "for row in rows:\n",
    "    cells=row.find_all('td')\n",
    "    if len(cells)>=6:\n",
    "    rank=cells[0].text.strip()\n",
    "    state=cells[1].text.strip()\n",
    "    Gdp_18_19=cells[2].text.strip()\n",
    "    Gdp_19_20=cells[3].text.strip()\n",
    "    share_18_19=cells[4].text.strip()\n",
    "    Gdp_billion=cells[5].text.strip()\n",
    "    data.append({'Rank':rank,'State':state,'GSDP(18-19)':Gdp_18_19,'GSDP(19-20)':Gdp_19_20,'Share(18-19)':share_18_19,'GDP($billion)':Gdp_billion})\n",
    "for item in data:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f57220",
   "metadata": {},
   "source": [
    "# 4. Scrape the details of trending repositories on Github.com. \n",
    "Url = https://github.com/ \n",
    "You have to find the following details: \n",
    "A) Repository title \n",
    "B) Repository description \n",
    "C) Contributors count \n",
    "D) Language used \n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url= 'https://github.com/ '\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "explore_menu=soup.find('nav',class_='UnderlineNav-body')\n",
    "trending_link=explore_menu.find('a',href='/trending')\n",
    "trending_url='https://github.com'+trending_link['href']\n",
    "trending_response=requests.get(trending_url)\n",
    "trending_soup=BeautifulSoup(trending_response.content,'html.parser')\n",
    "repositories=trending_soup.find_all('article',class_='Box-row')\n",
    "for repo in respositories:\n",
    "    title=repo.find('h1').text.strip()\n",
    "    description=repo.find('p',class_='col-9').text.strip()\n",
    "    contributors=repo.find('a',href=lambda href:href and '/contributors' in href)\n",
    "    contributors_count=contributors.text.strip() if contributors else '0'\n",
    "    language=repo.find('span',itemprop='programingLanguage')\n",
    "    language_used=language.text.strip() if language else 'Not specified'\n",
    "    print('Title:',title)\n",
    "    print('Description:',description)\n",
    "    print('Contributors count:',contributors_count)\n",
    "    print('language used:',language_used)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7336d48",
   "metadata": {},
   "source": [
    "# 5. Scrape the details of top 100 songs on billiboard.com. Url = https:/www.billboard.com/ You have to find the \n",
    "following details: \n",
    "A) Song name \n",
    "B) Artist name \n",
    "C) Last week rank \n",
    "D) Peak rank \n",
    "E) Weeks on board \n",
    " Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5aa3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url= 'https://www.billboard.com/ '\n",
    "response=requests.get(url)\n",
    "html_content=response.content\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "charts_link=soup.find('a',text='charts')\n",
    "hot100_link=charts_link['href']\n",
    "hot100_url=url+hot100_link\n",
    "hot100_response=requests.get(hot100_url)\n",
    "hot100_html_content=hot100_response.content\n",
    "hot100_soup=BeautifulSoup(hot100_html_content,'html.parser')\n",
    "table=hot100_soup.find('table',class_='chart-list')\n",
    "for row in table.find_all('tr'):\n",
    "    columns=row.find_all('td')\n",
    "    if len(columns)==5:\n",
    "        song_name=columns[1].text.strip()\n",
    "        artist_name=columns[2].text.strip()\n",
    "        last_week_rank=columns[3].text.strip()\n",
    "        peak_rank=columns[4].text.strip()\n",
    "        weeks_on_board=columns[5].test.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb67e8d",
   "metadata": {},
   "source": [
    "# 6. Scrape the details of Highest selling novels. \n",
    "A) Book name \n",
    "B) Author name \n",
    "C) Volumes sold \n",
    "D) Publisher \n",
    "E) Genre \n",
    " Url - https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfbc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as ap\n",
    "from bs4 import BeautifulSoup\n",
    "url='https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare'\n",
    "response=requests.get(url)\n",
    "soup=BeautifulSoup(response.content,'html.parser')\n",
    "#print(soup)\n",
    "novels=[]\n",
    "table=soup.find('table')\n",
    "rows=table.find_all('tr')[1:]\n",
    "#print(rows)\n",
    "for row in rows:\n",
    "    columns=row.find_all('td')\n",
    "book_name=columns[1].text.strip()\n",
    "author_name=columns[2].text.strip()\n",
    "volumes_sold=columns[3].text.strip()\n",
    "publisher=columns[4].text.strip()\n",
    "novels={'Book Name':book_name,'Author Name':author_name,'Volumes Sold':volumes_sold,'Publisher':publisher}\n",
    "print(novels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127aa8d",
   "metadata": {},
   "source": [
    "# 7. Scrape the details most watched tv series of all time from imdb.com. \n",
    "Url = https://www.imdb.com/list/ls095964455/ You have \n",
    "to find the following details: \n",
    "A) Name \n",
    "B) Year span \n",
    "C) Genre \n",
    "D) Run time \n",
    "E) Ratings \n",
    "F) Votes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c630bdeb",
   "metadata": {},
   "source": [
    "# 8. Details of Datasets from UCI machine learning repositories. \n",
    "Url = https://archive.ics.uci.edu/ You \n",
    "have to find the following details: \n",
    "A) Dataset name \n",
    "B) Data type \n",
    "C) Task \n",
    "D) Attribute type \n",
    "E) No of instances \n",
    "F) No of attribute G) Year \n",
    " Note: - from the home page you have to go to the Show All Dataset page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a1c16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013304a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
